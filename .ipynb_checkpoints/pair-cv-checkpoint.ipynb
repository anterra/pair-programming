{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static\n",
    "\n",
    "\n",
    "\n",
    "By now you're familiar with the typical use of linear regression: you're given a dataframe with a bunch of features and a set of observations for each feature. You then take one feature (your y) and try to predict it based on other features (your x's). \n",
    "\n",
    "This generally works to a certain extent. This morning, though, we're going to set up a rather strange problem for linear regression. We will see what happens when y and the x's are totally unrelated.\n",
    "\n",
    "Recall that regression follows the formula \n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\epsilon\n",
    "$$ \n",
    "When we \"fit\" an OLS model we are giving the model all of the x's and the y and asking it to find the best betas.\n",
    "\n",
    "This time, however, we're going to set all the betas to zero \n",
    "$$\n",
    "\\beta_0=\\beta_1=\\dots=0\n",
    "$$\n",
    "and then fit an OLS model.\n",
    "\n",
    "Before we do this, stop and think for a second:\n",
    "\n",
    "- What do you expect the model to do? What will the betas/r-squared/p-values that it finds look like?\n",
    "- What do you think the model *should* do? Is that different from what you think it *will* do?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/5a/No_Signal_23.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "Generate simulation data. We want to have 200 points (observations) for the y feature and for 20 x features. In other words make sure `y.shape == (200,1)` and `x.shape == (200,20)`. The x's should be randomly generated independent of each other. And the y should be randomly generated independent of the x's. \n",
    "\n",
    "Use statsmodels to fit an OLS model to your data. Are the results as you expected? Do you have any betas with a $p<0.05$? If not, re-run the model until you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.random.randn(200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(200):\n",
    "    noise = np.random.randint(500, size=(20))\n",
    "    data.append(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.DataFrame(target)\n",
    "df_y.rename(columns={0:\"target\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>153</td>\n",
       "      <td>71</td>\n",
       "      <td>298</td>\n",
       "      <td>110</td>\n",
       "      <td>141</td>\n",
       "      <td>19</td>\n",
       "      <td>352</td>\n",
       "      <td>415</td>\n",
       "      <td>274</td>\n",
       "      <td>...</td>\n",
       "      <td>301</td>\n",
       "      <td>328</td>\n",
       "      <td>235</td>\n",
       "      <td>2</td>\n",
       "      <td>207</td>\n",
       "      <td>118</td>\n",
       "      <td>32</td>\n",
       "      <td>124</td>\n",
       "      <td>22</td>\n",
       "      <td>-2.126509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>182</td>\n",
       "      <td>271</td>\n",
       "      <td>485</td>\n",
       "      <td>491</td>\n",
       "      <td>228</td>\n",
       "      <td>40</td>\n",
       "      <td>235</td>\n",
       "      <td>350</td>\n",
       "      <td>64</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>206</td>\n",
       "      <td>419</td>\n",
       "      <td>97</td>\n",
       "      <td>227</td>\n",
       "      <td>11</td>\n",
       "      <td>492</td>\n",
       "      <td>87</td>\n",
       "      <td>328</td>\n",
       "      <td>111</td>\n",
       "      <td>0.309988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>167</td>\n",
       "      <td>253</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>41</td>\n",
       "      <td>471</td>\n",
       "      <td>89</td>\n",
       "      <td>242</td>\n",
       "      <td>...</td>\n",
       "      <td>361</td>\n",
       "      <td>192</td>\n",
       "      <td>134</td>\n",
       "      <td>329</td>\n",
       "      <td>404</td>\n",
       "      <td>95</td>\n",
       "      <td>131</td>\n",
       "      <td>77</td>\n",
       "      <td>324</td>\n",
       "      <td>0.010627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>393</td>\n",
       "      <td>417</td>\n",
       "      <td>446</td>\n",
       "      <td>276</td>\n",
       "      <td>57</td>\n",
       "      <td>279</td>\n",
       "      <td>49</td>\n",
       "      <td>438</td>\n",
       "      <td>8</td>\n",
       "      <td>306</td>\n",
       "      <td>...</td>\n",
       "      <td>348</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>213</td>\n",
       "      <td>52</td>\n",
       "      <td>356</td>\n",
       "      <td>310</td>\n",
       "      <td>143</td>\n",
       "      <td>97</td>\n",
       "      <td>-1.476309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>336</td>\n",
       "      <td>147</td>\n",
       "      <td>120</td>\n",
       "      <td>93</td>\n",
       "      <td>63</td>\n",
       "      <td>172</td>\n",
       "      <td>229</td>\n",
       "      <td>302</td>\n",
       "      <td>138</td>\n",
       "      <td>311</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>260</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>292</td>\n",
       "      <td>241</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>418</td>\n",
       "      <td>-0.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>353</td>\n",
       "      <td>428</td>\n",
       "      <td>201</td>\n",
       "      <td>354</td>\n",
       "      <td>333</td>\n",
       "      <td>208</td>\n",
       "      <td>215</td>\n",
       "      <td>381</td>\n",
       "      <td>24</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>344</td>\n",
       "      <td>295</td>\n",
       "      <td>325</td>\n",
       "      <td>413</td>\n",
       "      <td>270</td>\n",
       "      <td>229</td>\n",
       "      <td>239</td>\n",
       "      <td>80</td>\n",
       "      <td>157</td>\n",
       "      <td>0.943571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>9</td>\n",
       "      <td>326</td>\n",
       "      <td>184</td>\n",
       "      <td>15</td>\n",
       "      <td>323</td>\n",
       "      <td>102</td>\n",
       "      <td>32</td>\n",
       "      <td>476</td>\n",
       "      <td>374</td>\n",
       "      <td>415</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>286</td>\n",
       "      <td>413</td>\n",
       "      <td>315</td>\n",
       "      <td>34</td>\n",
       "      <td>302</td>\n",
       "      <td>70</td>\n",
       "      <td>42</td>\n",
       "      <td>71</td>\n",
       "      <td>-0.904040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>133</td>\n",
       "      <td>436</td>\n",
       "      <td>468</td>\n",
       "      <td>444</td>\n",
       "      <td>297</td>\n",
       "      <td>416</td>\n",
       "      <td>414</td>\n",
       "      <td>391</td>\n",
       "      <td>100</td>\n",
       "      <td>451</td>\n",
       "      <td>...</td>\n",
       "      <td>299</td>\n",
       "      <td>231</td>\n",
       "      <td>128</td>\n",
       "      <td>63</td>\n",
       "      <td>197</td>\n",
       "      <td>330</td>\n",
       "      <td>444</td>\n",
       "      <td>9</td>\n",
       "      <td>68</td>\n",
       "      <td>-1.680813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>272</td>\n",
       "      <td>153</td>\n",
       "      <td>375</td>\n",
       "      <td>337</td>\n",
       "      <td>165</td>\n",
       "      <td>249</td>\n",
       "      <td>395</td>\n",
       "      <td>301</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>422</td>\n",
       "      <td>372</td>\n",
       "      <td>353</td>\n",
       "      <td>249</td>\n",
       "      <td>19</td>\n",
       "      <td>423</td>\n",
       "      <td>65</td>\n",
       "      <td>370</td>\n",
       "      <td>451</td>\n",
       "      <td>1.288991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>328</td>\n",
       "      <td>222</td>\n",
       "      <td>454</td>\n",
       "      <td>147</td>\n",
       "      <td>340</td>\n",
       "      <td>4</td>\n",
       "      <td>452</td>\n",
       "      <td>482</td>\n",
       "      <td>22</td>\n",
       "      <td>311</td>\n",
       "      <td>...</td>\n",
       "      <td>315</td>\n",
       "      <td>211</td>\n",
       "      <td>477</td>\n",
       "      <td>450</td>\n",
       "      <td>302</td>\n",
       "      <td>406</td>\n",
       "      <td>280</td>\n",
       "      <td>493</td>\n",
       "      <td>160</td>\n",
       "      <td>0.107739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9  ...   11   12   13  \\\n",
       "0    475  153   71  298  110  141   19  352  415  274  ...  301  328  235   \n",
       "1    182  271  485  491  228   40  235  350   64   56  ...  206  419   97   \n",
       "2    101  167  253  386    1  169   41  471   89  242  ...  361  192  134   \n",
       "3    393  417  446  276   57  279   49  438    8  306  ...  348   88   88   \n",
       "4    336  147  120   93   63  172  229  302  138  311  ...    6  260   50   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "195  353  428  201  354  333  208  215  381   24   53  ...  344  295  325   \n",
       "196    9  326  184   15  323  102   32  476  374  415  ...   18  286  413   \n",
       "197  133  436  468  444  297  416  414  391  100  451  ...  299  231  128   \n",
       "198  272  153  375  337  165  249  395  301  484    0  ...  422  372  353   \n",
       "199  328  222  454  147  340    4  452  482   22  311  ...  315  211  477   \n",
       "\n",
       "      14   15   16   17   18   19    target  \n",
       "0      2  207  118   32  124   22 -2.126509  \n",
       "1    227   11  492   87  328  111  0.309988  \n",
       "2    329  404   95  131   77  324  0.010627  \n",
       "3    213   52  356  310  143   97 -1.476309  \n",
       "4     58  292  241  128    3  418 -0.599998  \n",
       "..   ...  ...  ...  ...  ...  ...       ...  \n",
       "195  413  270  229  239   80  157  0.943571  \n",
       "196  315   34  302   70   42   71 -0.904040  \n",
       "197   63  197  330  444    9   68 -1.680813  \n",
       "198  249   19  423   65  370  451  1.288991  \n",
       "199  450  302  406  280  493  160  0.107739  \n",
       "\n",
       "[200 rows x 21 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "std.fit(X_train.values)\n",
    "X_train_scaled = std.transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y_train, X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared (uncentered):</th>      <td>   0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   1.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 31 Jul 2020</td> <th>  Prob (F-statistic):</th>           <td> 0.417</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:44:39</td>     <th>  Log-Likelihood:    </th>          <td> -228.66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   160</td>      <th>  AIC:               </th>          <td>   497.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   140</td>      <th>  BIC:               </th>          <td>   558.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    20</td>      <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0903</td> <td>    0.091</td> <td>    0.989</td> <td> 0.324</td> <td>   -0.090</td> <td>    0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>  <td>   -0.1597</td> <td>    0.092</td> <td>   -1.733</td> <td> 0.085</td> <td>   -0.342</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>  <td>    0.0269</td> <td>    0.090</td> <td>    0.299</td> <td> 0.766</td> <td>   -0.151</td> <td>    0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>  <td>    0.0407</td> <td>    0.094</td> <td>    0.435</td> <td> 0.665</td> <td>   -0.144</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>  <td>    0.2135</td> <td>    0.092</td> <td>    2.313</td> <td> 0.022</td> <td>    0.031</td> <td>    0.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>  <td>   -0.0394</td> <td>    0.092</td> <td>   -0.427</td> <td> 0.670</td> <td>   -0.222</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>  <td>    0.1078</td> <td>    0.089</td> <td>    1.216</td> <td> 0.226</td> <td>   -0.067</td> <td>    0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>  <td>    0.0492</td> <td>    0.093</td> <td>    0.527</td> <td> 0.599</td> <td>   -0.135</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>  <td>    0.0909</td> <td>    0.091</td> <td>    0.994</td> <td> 0.322</td> <td>   -0.090</td> <td>    0.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th> <td>   -0.1048</td> <td>    0.090</td> <td>   -1.161</td> <td> 0.248</td> <td>   -0.283</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th> <td>   -0.0029</td> <td>    0.090</td> <td>   -0.032</td> <td> 0.975</td> <td>   -0.181</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th> <td>   -0.1002</td> <td>    0.091</td> <td>   -1.102</td> <td> 0.272</td> <td>   -0.280</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th> <td>   -0.1453</td> <td>    0.090</td> <td>   -1.622</td> <td> 0.107</td> <td>   -0.322</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th> <td>    0.0159</td> <td>    0.091</td> <td>    0.174</td> <td> 0.862</td> <td>   -0.164</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th> <td>    0.0344</td> <td>    0.092</td> <td>    0.373</td> <td> 0.710</td> <td>   -0.148</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th> <td>   -0.0626</td> <td>    0.090</td> <td>   -0.695</td> <td> 0.488</td> <td>   -0.241</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th> <td>    0.1702</td> <td>    0.090</td> <td>    1.881</td> <td> 0.062</td> <td>   -0.009</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th> <td>    0.0512</td> <td>    0.088</td> <td>    0.580</td> <td> 0.563</td> <td>   -0.123</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th> <td>   -0.0165</td> <td>    0.089</td> <td>   -0.186</td> <td> 0.853</td> <td>   -0.193</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th> <td>    0.0586</td> <td>    0.089</td> <td>    0.661</td> <td> 0.510</td> <td>   -0.117</td> <td>    0.234</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.747</td> <th>  Durbin-Watson:     </th> <td>   1.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.688</td> <th>  Jarque-Bera (JB):  </th> <td>   0.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.042</td> <th>  Prob(JB):          </th> <td>   0.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.659</td> <th>  Cond. No.          </th> <td>    1.80</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                 target   R-squared (uncentered):                   0.130\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.005\n",
       "Method:                 Least Squares   F-statistic:                              1.043\n",
       "Date:                Fri, 31 Jul 2020   Prob (F-statistic):                       0.417\n",
       "Time:                        09:44:39   Log-Likelihood:                         -228.66\n",
       "No. Observations:                 160   AIC:                                      497.3\n",
       "Df Residuals:                     140   BIC:                                      558.8\n",
       "Df Model:                          20                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0903      0.091      0.989      0.324      -0.090       0.271\n",
       "x2            -0.1597      0.092     -1.733      0.085      -0.342       0.022\n",
       "x3             0.0269      0.090      0.299      0.766      -0.151       0.205\n",
       "x4             0.0407      0.094      0.435      0.665      -0.144       0.226\n",
       "x5             0.2135      0.092      2.313      0.022       0.031       0.396\n",
       "x6            -0.0394      0.092     -0.427      0.670      -0.222       0.143\n",
       "x7             0.1078      0.089      1.216      0.226      -0.067       0.283\n",
       "x8             0.0492      0.093      0.527      0.599      -0.135       0.234\n",
       "x9             0.0909      0.091      0.994      0.322      -0.090       0.272\n",
       "x10           -0.1048      0.090     -1.161      0.248      -0.283       0.074\n",
       "x11           -0.0029      0.090     -0.032      0.975      -0.181       0.175\n",
       "x12           -0.1002      0.091     -1.102      0.272      -0.280       0.080\n",
       "x13           -0.1453      0.090     -1.622      0.107      -0.322       0.032\n",
       "x14            0.0159      0.091      0.174      0.862      -0.164       0.196\n",
       "x15            0.0344      0.092      0.373      0.710      -0.148       0.217\n",
       "x16           -0.0626      0.090     -0.695      0.488      -0.241       0.115\n",
       "x17            0.1702      0.090      1.881      0.062      -0.009       0.349\n",
       "x18            0.0512      0.088      0.580      0.563      -0.123       0.226\n",
       "x19           -0.0165      0.089     -0.186      0.853      -0.193       0.160\n",
       "x20            0.0586      0.089      0.661      0.510      -0.117       0.234\n",
       "==============================================================================\n",
       "Omnibus:                        0.747   Durbin-Watson:                   1.889\n",
       "Prob(Omnibus):                  0.688   Jarque-Bera (JB):                0.823\n",
       "Skew:                           0.042   Prob(JB):                        0.663\n",
       "Kurtosis:                       2.659   Cond. No.                         1.80\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Now, automate the process! Run the above analysis but vary the number of x's from 1 to 200. Log the r2 and r2-adj for each case and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingDataError",
     "evalue": "exog contains inf or nans",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDataError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-6549cd759103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mr_squared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsquared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m                  **kwargs):\n\u001b[1;32m    858\u001b[0m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 859\u001b[0;31m                                   hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 702\u001b[0;31m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pinv_wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wendog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 77\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 672\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_handle_constant\u001b[0;34m(self, hasconst)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mexog_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mMissingDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exog contains inf or nans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mexog_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog_max\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexog_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingDataError\u001b[0m: exog contains inf or nans"
     ]
    }
   ],
   "source": [
    "r2 = []\n",
    "for i in range(400): \n",
    "    noise = np.random.randint(200, size=(i))\n",
    "    target = np.random.randint(200, size=(i))\n",
    "    df = pd.DataFrame(data)\n",
    "    df_y = pd.DataFrame(target)\n",
    "    df_y.rename(columns={0:\"target\"}, inplace=True)\n",
    "    df = pd.concat([df, df_y], axis=1)\n",
    "    X = df.iloc[:, 0:-1]\n",
    "    y = df.iloc[:, -1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    std = StandardScaler()\n",
    "    std.fit(X_train.values)\n",
    "    X_train_scaled = std.transform(X_train.values)\n",
    "    model = sm.OLS(y_train, X_train_scaled)\n",
    "    fit = model.fit()\n",
    "    r_squared = fit.rsquared\n",
    "    r2.append(r_squared)\n",
    "    \n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
